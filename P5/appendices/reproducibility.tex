

\chapter{Reproducibility self-assessment}
I have made all code and raw benchmarking results available on GitHub, and have added the sources to the datasets that I have used.
This means that anyone could create the same compressed datasets as I have.
Dataset characteristics and theoretical results could thus be entirely reproduced.
However, the benchmarking results have some uncerainties in them, despite always being iterated for 10 times.
It is dependent on computer specifications and mostly randomness (at least to me---I can not completely explain the inner workings of a server, browser, and computer).
So it is not possible to exactly recreate these results, but if every test is iterated in the same way with similar computer specifications, it is very likely that the results are close enough for the analysis to remain the same.

But I could have maintained my code better on GitHub and in general.
Parts are hard to read and not always properly commented, or have outdated ones.
So if one is to not reuse my code and do the research from scratch (only looking at my code rather than copying), it is likely that there will be some differences in the recreation.
